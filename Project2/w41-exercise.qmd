---
title: Exercises week 41
subtitle: FYS-STK4155
author: Even Sletteng Garvang
format: pdf
---

```{python}
#| label: setup
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
```

Here I implement various methods of gradient descent (GD) for a simple synthetic dataset generated by a polynomial function.

Generate data

```{python}
def data_function(x):
    return 4*x**3 + x**2 - 17*x + 48

np.random.seed(8923)

# generate data
n = 100
x = np.linspace(-2, 2, n)
f_x = data_function(x)
y = f_x + np.random.normal(0, 1, n)
y = y.reshape(-1,1)

# create design matrix of polynomials
# for now 3rd order reflecting data function
X = PolynomialFeatures(3).fit_transform(x.reshape(-1, 1))

plt.plot(x, y, "o")
plt.show()
```



## Plain gradient descent

```{python}
def gradient_OLS(X, y, theta):
    n = y.shape[0]
    return -(2.0/n) * X.T @ (y - X @ theta)

def gradient_descent(eta, X, y, n_iter, gradient_fun=gradient_OLS, check_converge=False, thresh=0.001):
    theta = np.random.randn(X.shape[1], 1)
    for i in range(n_iter):
        gradient = gradient_fun(X, y, theta)
        if check_converge:
            if np.all(np.abs(gradient) < thresh):
                print(f"Converged for eta={round(eta, 3)} after {i+1} iterations.")
                return theta
        theta -= eta * gradient
    if check_converge:
        print(f"Did not converge for eta={round(eta, 3)} and {n_iter} iterations.")
    return theta


np.random.seed(50)
etas = np.linspace(0.01, 0.1, 10)
n_iter = 1000
for eta in etas:
    theta = gradient_descent(eta, X, y, n_iter, check_converge=True)
    
```

A learning rate of 0.09 works best for this data, converging after 254 iterations. This was with convergence criterion of all gradients being smaller than 0.001.

```{python}
theta_analytical = np.linalg.inv(X.T @ X) @ X.T @ y
np.random.seed(50)
eta = 0.09
n_iter = 254
theta = gradient_descent(eta, X, y, n_iter)

print("Analytical:")
print(theta_analytical)
print("Gradient descent (plain):")
print(theta)
```

## Gradient descent with momentum

```{python}
# function for momentum
def momentum_change(eta, gradient, gamma, change):
    return eta * gradient + gamma * change

# modify for momentum function
def gradient_descent_momentum(eta, X, y, n_iter, gamma, gradient_fun=gradient_OLS, check_converge=False, thresh=0.001):
    theta = np.random.randn(X.shape[1], 1)
    change = 0
    for i in range(n_iter):
        gradient = gradient_fun(X, y, theta)
        if check_converge:
            if np.all(np.abs(gradient) < thresh):
                print(f"Converged for eta={round(eta, 3)} after {i+1} iterations.")
                return theta
        new_change = momentum_change(eta, gradient, gamma, change)
        change = new_change
        theta -= new_change

    if check_converge:
        print(f"Did not converge for eta={round(eta, 3)} and {n_iter} iterations.")
    return theta

np.random.seed(50)
gamma = 0.7
n_iter = 200
np.random.seed(50)
etas = np.linspace(0.01, 0.1, 10)
n_iter = 1000
for eta in etas:
    theta = gradient_descent_momentum(eta, X, y, n_iter, gamma, check_converge=True)
```

For $\gamma=0.7$, the algorithm converges much faster than without momentum, and converges for the whole range of learning rates.

## Stochastic gradient descent

We implement stochastic gradient descent, which helps with avoiding getting stuck in local minima.

```{python}
def SGD(X, y, eta, gradient_fun, n_epochs, M):
    n = y.shape[0]
    m = int(n/M)
    xy = np.column_stack([X,y]) # for shuffling x and y together
    theta = np.random.randn(X.shape[1], 1)

    for i in range(n_epochs):
        np.random.shuffle(xy)
        for j in range(m):
            random_index = M * np.random.randint(m)
            xi = xy[random_index:random_index+5, :-1]
            yi = xy[random_index:random_index+5, -1:]
            gradient = (1/M)*gradient_fun(xi, yi, theta)
            theta = theta - eta*gradient
    return theta

np.random.seed(39)
eta = 0.01
n_epochs = 200
M = 5        # minibatch size


theta = SGD(X, y, eta, gradient_OLS, n_epochs, M)

print(theta)
print(theta_analytical)

```

### SGD with momentum


```{python}
def SGD_momentum(X, y, eta, gradient_fun, n_epochs, M, gamma):
    n = y.shape[0]
    m = int(n/M)
    xy = np.column_stack([X,y]) # for shuffling x and y together
    theta = np.random.randn(X.shape[1], 1)

    change = 0
    for i in range(n_epochs):
        np.random.shuffle(xy)
        for j in range(m):
            random_index = M * np.random.randint(m)
            xi = xy[random_index:random_index+5, :-1]
            yi = xy[random_index:random_index+5, -1:]
            gradient = (1/M)*gradient_fun(xi, yi, theta)
            new_change = momentum_change(eta, gradient, gamma, change)
            theta = theta - new_change
            change = new_change
    return theta

np.random.seed(60)
eta = 0.01
gamma = 0.3
n_epochs = 100
M = 5        # minibatch size

theta = SGD_momentum(X, y, eta, gradient_OLS, n_epochs, M, gamma)

print(theta)
print(theta_analytical)
```

## AdaGrad

```{python}
def AdaGrad(gradient, Giter, eta, delta = 1e-8):
    Giter += gradient*gradient
    update = gradient * eta / (delta + np.sqrt(Giter))
    return Giter, update

def GD_AdaGrad(eta, X, y, n_iter, gradient_fun=gradient_OLS):
    theta = np.random.randn(X.shape[1], 1)
    Giter = 0
    for i in range(n_iter):
        gradient = gradient_fun(X, y, theta)
        Giter, update = AdaGrad(gradient, Giter, eta)
        theta -= update
    return theta

eta = 0.5
n_iter = 200

theta = GD_AdaGrad(eta, X, y, n_iter)
print(theta)
print(theta_analytical)
```

## RMSProp


```{python}
def RMSProp(gradient, Giter, eta, rho, delta=1e-8):
    Giter = rho*Giter + (1-rho)*gradient*gradient
    update = gradient*eta/(delta+np.sqrt(Giter))
    return Giter, update

def GD_RMSProp(eta, X, y, n_iter, gradient_fun=gradient_OLS):
    theta = np.random.randn(X.shape[1], 1)
    Giter = 0
    for i in range(n_iter):
        gradient = gradient_fun(X, y, theta)
        Giter, update = RMSProp(gradient, Giter, eta, rho)
        theta -= eta * gradient
    return theta
```

## General GD function


```{python}
# function for momentum
def momentum_change(eta, gradient, gamma, change):
    return eta * gradient + gamma * change

def AdaGrad(update_term, gradient, Giter, delta = 1e-8):
    Giter += gradient*gradient
    update = update_term / (delta + np.sqrt(Giter))
    return Giter, update

def RMSProp(update_term, gradient, Giter, rho, delta=1e-8):
    Giter = rho*Giter + (1-rho)*gradient*gradient
    update = update_term / (delta+np.sqrt(Giter))
    return Giter, update

def ADAM(gradient, first_moment, second_moment, beta1, beta2, itr, delta=1e-8):
    first_moment = beta1*first_moment + (1-beta1)*gradient
    second_moment = beta2*second_moment + (1-beta2)*gradient*gradient

    first_term = first_moment/(1.0-beta1**itr)
    second_term = second_moment/(1.0-beta2**itr)
    update = eta*first_term/(np.sqrt(second_term)+delta)

    return first_moment, second_moment, update

def GD_inner(eta, theta, moments, gradient, momentum=False, gamma=None, adaptive_fun=None, adam = False, **kwargs):
    adaptive = adaptive_fun is not None

    if adam:
        first_moment, second_moment = moments
        first_moment, second_moment, update = ADAM(gradient, first_moment, second_moment, **kwargs)
        theta -= update
        return theta, first_moment, second_moment
    else:
        Giter, change = moments
        update = eta * gradient
        if momentum:
            update += gamma * change
            change = update
        if adaptive:
            Giter, update = adaptive_fun(update, gradient, Giter, **kwargs)
    
    theta -= update
    return theta, Giter, change

def GD(X, y, eta, n_iter, gradient_fun=gradient_OLS, momentum=False, gamma=None, adaptive_fun=None, adam=False, **kwargs):
    theta = np.random.randn(X.shape[1], 1)
    
    # moment 1 and 2 of ADAM
    # Giter and change if not ADAM
    moments = [0, 0]

    for i in range(n_iter):
        gradient = gradient_fun(X, y, theta)
        theta, moments[0], moments[1] = GD_inner(eta, theta, moments, gradient, momentum, gamma, adaptive_fun, adam, **kwargs, itr = i+1)
    return theta

def SGD(X, y, eta, M, n_epochs, gradient_fun=gradient_OLS, momentum=False, gamma=None, adaptive_fun=None, adam=False, **kwargs):
    n = y.shape[0]
    m = int(n/M)
    xy = np.column_stack([X,y]) # for shuffling x and y together
    theta = np.random.randn(X.shape[1], 1)
    # moment 1 and 2 of ADAM
    # Giter and change if not ADAM
    moments = [0, 0]

    for i in range(n_epochs):
        Giter = 0.0
        np.random.shuffle(xy)
        for j in range(m):
            random_index = M * np.random.randint(m)
            xi = xy[random_index:random_index+5, :-1]
            yi = xy[random_index:random_index+5, -1:]
            gradient = (1/M)*gradient_fun(xi, yi, theta)
            theta, moments[0], moments[1] = GD_inner(eta, theta, moments, gradient, momentum, gamma, adaptive_fun, adam, **kwargs, itr = i+1)
    return theta
```

```{python}
eta = 0.5
n_iter = 1000
beta1 = 0.9
beta2 = 0.999
theta = GD(X, y, eta, n_iter, adam=True, beta1=beta1, beta2=beta2)
print(theta)

M = 5
n_epochs = 100
theta_sgd_adam = SGD(X, y, eta, M, n_epochs, adam=True, beta1=beta1, beta2=beta2)

print(theta_sgd_adam)
```

```{python}
eta = 0.09
n_iter = 100


theta = GD(X, y, eta, n_iter)
np.random.seed(56)
theta_momentum = GD(X, y, eta, n_iter, momentum=True, gamma = 0.3)
np.random.seed(56)
theta_momentum_old = gradient_descent_momentum(eta, X, y, n_iter, gamma=0.3)

print(theta)
print(theta_momentum)
print(theta_momentum_old)


theta_adagrad = GD(X, y, eta=10, n_iter=n_iter, adaptive_fun=AdaGrad)
theta_adagrad_momentum = GD(X, y, eta=10, n_iter=n_iter, momentum=True, gamma = 0.3, adaptive_fun=AdaGrad)




print("Adagrad:")
print(theta_adagrad)
print(theta_adagrad_momentum)

theta_rmsprop = GD(X, y, eta=10, n_iter=n_iter, adaptive_fun=RMSProp, rho=0.99)
print("RMSProp")
print(theta_rmsprop)
```


```{python}
eta = 0.09
M = 5
n_epochs = 50
# test SGD
theta = SGD(X, y, eta, M, n_epochs)
theta_momentum = SGD(X, y, eta, M, n_epochs, momentum=True, gamma = 0.3)

print(theta)
print(theta_momentum)

eta = 10
theta_adagrad = SGD(X, y, eta, M, n_epochs, adaptive_fun=AdaGrad)
theta_adagrad_momentum = SGD(X, y, eta, M, n_epochs, momentum=True, gamma = 0.3, adaptive_fun=AdaGrad)

print("Adagrad:")
print(theta_adagrad)
print(theta_adagrad_momentum)
```