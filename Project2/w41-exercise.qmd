---
title: Exercises week 41
subtitle: FYS-STK4155
author: Even Sletteng Garvang
format: pdf
---

```{python}
#| label: setup
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
```

Here I implement various methods of gradient descent (GD) for a simple synthetic dataset generated by a polynomial function.

Generate data

```{python}
def data_function(x):
    return 4*x**3 + x**2 - 17*x + 48

np.random.seed(8923)

# generate data
n = 100
x = np.linspace(-2, 2, n)
f_x = data_function(x)
y = f_x + np.random.normal(0, 1, n)
y = y.reshape(-1,1)

# create design matrix of polynomials
# for now 3rd order reflecting data function
X = PolynomialFeatures(3).fit_transform(x.reshape(-1, 1))

plt.plot(x, y, "o")
plt.show()
```



Na√Øve implementation of gradient descent

```{python}
def gradient_OLS(X, y, theta, n):
    return -(2.0/n) * X.T @ (y - X @ theta)

XT_X = X.T @ X
theta_analytical = np.linalg.inv(XT_X) @ X.T @ y

np.random.seed(37)
theta = np.random.randn(X.shape[1], 1)
eta = 0.09

n_iter = 200
for _ in range(n_iter):
    gradient = gradient_OLS(X, y, theta, n)
    theta -= eta * gradient 


print("Analytical:")
print(theta_analytical)
print("Gradient descent (plain):")
print(theta)
```

Here I tried many different learning rates before landing on 0.09 (not shown).

