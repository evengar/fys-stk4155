---
title: Exercises week 41
subtitle: FYS-STK4155
author: Even Sletteng Garvang
format: pdf
---

```{python}
#| label: setup
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
```

Here I implement various methods of gradient descent (GD) for a simple synthetic dataset generated by a polynomial function.

Generate data

```{python}
def data_function(x):
    return 4*x**3 + x**2 - 17*x + 48

np.random.seed(8923)

# generate data
n = 100
x = np.linspace(-2, 2, n)
f_x = data_function(x)
y = f_x + np.random.normal(0, 1, n)
y = y.reshape(-1,1)

# create design matrix of polynomials
# for now 3rd order reflecting data function
X = PolynomialFeatures(3).fit_transform(x.reshape(-1, 1))

plt.plot(x, y, "o")
plt.show()
```



## Plain gradient descent

```{python}
def gradient_OLS(X, y, theta):
    n = y.shape[0]
    return -(2.0/n) * X.T @ (y - X @ theta)

def gradient_descent(eta, X, y, n_iter, gradient_fun=gradient_OLS, check_converge=False, thresh=0.001):
    theta = np.random.randn(X.shape[1], 1)
    for i in range(n_iter):
        gradient = gradient_fun(X, y, theta)
        if check_converge:
            if np.all(np.abs(gradient) < thresh):
                print(f"Converged for eta={round(eta, 3)} after {i+1} iterations.")
                return theta
        theta -= eta * gradient
    if check_converge:
        print(f"Did not converge for eta={round(eta, 3)} and {n_iter} iterations.")
    return theta


np.random.seed(50)
etas = np.linspace(0.01, 0.1, 10)
n_iter = 1000
for eta in etas:
    theta = gradient_descent(eta, X, y, n_iter, check_converge=True)
    
```

A learning rate of 0.09 works best for this data, converging after 254 iterations. This was with convergence criterion of all gradients being smaller than 0.001.

```{python}
theta_analytical = np.linalg.inv(X.T @ X) @ X.T @ y
np.random.seed(50)
eta = 0.09
n_iter = 254
theta = gradient_descent(eta, X, y, n_iter)

print("Analytical:")
print(theta_analytical)
print("Gradient descent (plain):")
print(theta)
```

## Gradient descent with momentum

```{python}
# function for momentum
def momentum_change(eta, gradient, gamma, change):
    return eta * gradient + gamma * change

# modify for momentum function
def gradient_descent_momentum(eta, X, y, n_iter, gamma, gradient_fun=gradient_OLS, check_converge=False, thresh=0.001):
    theta = np.random.randn(X.shape[1], 1)
    change = 0
    for i in range(n_iter):
        gradient = gradient_fun(X, y, theta)
        if check_converge:
            if np.all(np.abs(gradient) < thresh):
                print(f"Converged for eta={round(eta, 3)} after {i+1} iterations.")
                return theta
        new_change = momentum_change(eta, gradient, gamma, change)
        change = new_change
        theta -= new_change

    if check_converge:
        print(f"Did not converge for eta={round(eta, 3)} and {n_iter} iterations.")
    return theta

np.random.seed(50)
gamma = 0.7
n_iter = 200
np.random.seed(50)
etas = np.linspace(0.01, 0.1, 10)
n_iter = 1000
for eta in etas:
    theta = gradient_descent_momentum(eta, X, y, n_iter, gamma, check_converge=True)
```

For $\gamma=0.7$, the algorithm converges much faster than without momentum, and converges for the whole range of learning rates.

## Stochastic gradient descent

We implement stochastic gradient descent, which helps with avoiding getting stuck in local minima.

```{python}
def SGD(X, y, eta, gradient_fun, n_epochs, M):
    n = y.shape[0]
    m = int(n/M)
    xy = np.column_stack([X,y]) # for shuffling x and y together
    theta = np.random.randn(X.shape[1], 1)

    for i in range(n_epochs):
        np.random.shuffle(xy)
        for j in range(m):
            random_index = M * np.random.randint(m)
            xi = xy[random_index:random_index+5, :-1]
            yi = xy[random_index:random_index+5, -1:]
            gradient = gradient_fun(xi, yi, theta)
            theta = theta - eta*gradient
    return theta

np.random.seed(39)
eta = 0.01
n_epochs = 100
M = 5        # minibatch size


theta = SGD(X, y, eta, gradient_OLS, n_epochs, M)

print(theta)
print(theta_analytical)

```

### SGD with momentum


```{python}
def SGD_momentum(X, y, eta, gradient_fun, n_epochs, M, gamma):
    n = y.shape[0]
    m = int(n/M)
    xy = np.column_stack([X,y]) # for shuffling x and y together
    theta = np.random.randn(X.shape[1], 1)

    change = 0
    for i in range(n_epochs):
        np.random.shuffle(xy)
        for j in range(m):
            random_index = M * np.random.randint(m)
            xi = xy[random_index:random_index+5, :-1]
            yi = xy[random_index:random_index+5, -1:]
            gradient = gradient_fun(xi, yi, theta)
            new_change = momentum_change(eta, gradient, gamma, change)
            theta = theta - new_change
            change = new_change
    return theta

np.random.seed(60)
eta = 0.01
gamma = 0.3
n_epochs = 50
M = 5        # minibatch size

theta = SGD_momentum(X, y, eta, gradient_OLS, n_epochs, M, gamma)

print(theta)
print(theta_analytical)
```